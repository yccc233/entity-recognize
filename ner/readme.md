
# 新冠相关的医学实体命名抽取

## 数据已经训练完成，训练时间约半小时——MacOS

## 环境

pip安装方法给出：

```shell
pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.14.0-py3-none-any.whl
# 以下两个包暂时无用
pip3 install zhon -i https://pypi.tuna.tsinghua.edu.cn/simple
pip3 install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**注意，tensorflow的版本最好是1.14版本的**


**开发环境python支持3.6。不符合此条件的在main时会出错**


## 使用方法

1. tran.py运行可训练
2. main.py运行即可，string字符串可自定义，返回json串


---
# 原理：

### 训练参数配置

- flags参数：train.py中
	1. 训练批次	（train_epoch）：50
	2. 批次大小字符数	（batch_size）：32
	3. seg张量（seg_dim）：20
	4. char张量（char_dim）：100
	5. lstm张量（lstm_dim）：100
	6. 最大句长（max_seq_len）：64
	7. 模型保存地址（ckpt_path）：path
	8. 每次训练数量（clip）：5
	...



### 数据处理

初始数据文本:`./data/time.train`

1. **创建id和标签的映射文件**

-t 保存的是标记出现的次数，字典类型，如：
```
{0: 'O', 1: 'I-DISEASE', 2: 'I-PHEN', 3: 'B-PHEN', 4: 'I-COVID', 5: 'I-PROTEIN', 6: 'B-DISEASE', 7: 'I-GENE', 8: 'B-GENE', 9: 'B-PROTEIN', 10: 'B-COVID', 11: '[CLS]', 12: '[SEP]', 13: 'I-PTEN', 14: 'B-PTEN', 15: 'I-DISEADE'}
```

tag_to_id 保存的是标记对应的id值，按照出现频率排序，也是字典类型，如：

```
{'O': 0, 'I-DISEASE': 1, 'I-PHEN': 2, 'B-PHEN': 3, 'I-COVID': 4, 'I-PROTEIN': 5, 'B-DISEASE': 6, 'I-GENE': 7, 'B-GENE': 8, 'B-PROTEIN': 9, 'B-COVID': 10, '[CLS]': 11, '[SEP]': 12, 'I-PTEN': 13, 'B-PTEN': 14, 'I-DISEADE': 15}
```

id_to_tag同上，略

最后将lable-id保存到`maps.pkl`文件下。



2. **准备数据**

train_date 得到



包括每句话的列表（以下仅包含第一句的例子）

string：			句单字的列表

segment_ids：	暂定0，utils.py：235附近可见`segment_ids.append(0)`

ids				每个字的索引（./ner/albert-tiny/vocab.txt中的索引）

mask：			每个字的长度*[1]再补全数据

label_ids：		每个字对应的标签id

data[0]数据如下：
```
[[['利', '用', '单', '细', '胞', 'R', 'N', 'A', '测', '序', '分', '析', '技', '术', '，', '对', '新', '型', '冠', '状', '病', '毒', '2', '0', '1', '9', '-', 'n', 'C', 'O', 'V', '的', '受', '体', '基', '因', '血', '管', '紧', '张', '素', '转', '化', '酶', '2', '在', '人', '体', '肺', '脏', '内', '每', '一', '个', '细', '胞', '的', '表', '达', '情', '况', '进', '行', '了', '分', '析', '，', '研', '究', '了', '共', '计', '四', '万', '三', '千', '多', '个', '细', '胞', '。'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1164, 4500, 1296, 5301, 5528, 160, 156, 143, 3844, 2415, 1146, 3358, 2825, 3318, 8024, 2190, 3173, 1798, 1094, 4307, 4567, 3681, 123, 121, 122, 130, 118, 156, 145, 157, 164, 4638, 1358, 860, 1825, 1728, 6117, 5052, 5165, 2476, 5162, 6760, 1265, 6998, 123, 1762, 782, 860, 5511, 5552, 1079, 3680, 671, 702, 5301, 5528, 4638, 6134, 6809, 2658, 1105, 6822, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 9, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12]], [['已', '有', '研', '究', '表', '明', '，', '部', '分', '患', '者', '除', '呼', '吸', '系', '统', '损', '伤', '外', '，', '还', '存', '在', '肾', '功', '能', '异', '常', '甚', '至', '肾', '损', '伤', '，', '其', '相', '关', '机', '制', '尚', '不', '清', '楚', '。'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2347, 3300, 4777, 4955, 6134, 3209, 8024, 6956, 1146, 2642, 5442, 7370, 1461, 1429, 5143, 5320, 2938, 839, 1912, 8024, 6820, 2100, 1762, 5513, 1216, 5543, 2460, 2382, 4493, 5635, 5513, 2938, 839, 8024, 1071, 4685, 1068, 3322, 1169, 2213, 679, 3926, 3504, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 6, 1, 1, 1, 1, 0, 0, 6, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]
```

**tran_batch = 批/tran_data**
且tran_batch已经将句长度顺序排列完成，减少数据训练时维度跨度过大的是时间损失


### 准备训练——创建模型




### 训练



---
暂时不写啦～

